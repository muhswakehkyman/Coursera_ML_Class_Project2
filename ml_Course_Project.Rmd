---
title: "Practical Machine Learning Course Project"
date: "Sunday, September 14, 2014"
output: html_document
---

The goal of this project was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and accurately predict how the lift was performed. In the data set this would be the 'classe' variable.

When loading in the data, I replaced all fields with '#DIV/0!', blanks and NA's to be imported as R's default N/A value.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
library(randomForest)
library(Hmisc)
library(doParallel)
library(foreach)
 
training <- read.csv("~/pml-training.csv",na.strings=c("#DIV/0!", "","NA") )
testing <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!", "","NA") )
```

Data Manipulation
-------------------------
The original data set includes a low of missing data. My approach to this analysis is to use a complete data set. There is a lot of information in the data set even if we exclude the non-complete portions. I also removed columns that would not be useful in predictions, such as time-stamps, windows, and user names. I also split the training set in order for cross-validation. 

```{r}
#Limiting down variables to only complete data sets
#Remove columns with N/As
training <- training[,which(colSums(!is.na(training))==19622)]
testing <- testing[,which(colSums(!is.na(testing))==20)]
 
#user names and timestamps will not help in predicting performance
training <- training[,c(8:60)]
testing <- testing[,c(8:60)]

#Partition rows into training and crossvalidation
inTrain <- createDataPartition(training$classe, p = 0.6)[[1]]
crossv <- training[-inTrain,]
training <- training[ inTrain,]
```

Model Selection
---------------------------------
I decided to cut down the training set dramatically in order to be able to use random forests on my computer. I had a lot of issues trying to run random forest models on the complete data set. In order to optimize run time of the random forest model. I only attempted 6 random forests with 40 trees each.

```{r}
#Data cutdown for rf to process quickly
set.seed(12345)
training2 <- training[1:300,]
registerDoParallel()
x <- training[-ncol(training)]
y <- training$classe
rf <- foreach(ntree=rep(40, 6), .combine=randomForest::combine, .packages='randomForest') %dopar% {
randomForest(x, y, ntree=ntree) 
}
```

Results
---------------------------
The random forest model's results on the training set is 100% accurate. The prediction results for the random forest model against the cross validation data set was 99.4% accurate. We'd expect an out of sample error to be within the 95% confidence interval of the prediction accuracy. Thus the out of sample error rate should be between 0.4% and 0.8%.

Given these results, I am satisfied with this random forest model especially given how it was optimized for run time on my computer by using only a small portion of the data set. 
```{r,echo=TRUE, message=FALSE, warning=FALSE}
predictions1 <- predict(rf, newdata=training)
confusionMatrix(predictions1,training$classe)

predictions2 <- predict(rf, newdata=crossv)
confusionMatrix(predictions2,crossv$classe)
```

Testing Set Predictions and Courersa Submission Code
-----------------------------------------------------
Below is the submission results using the Coursera provided code.
```{r}
answers = predict(rf, newdata=testing)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(answers)
```
